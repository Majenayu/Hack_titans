Nice — this is a great project idea. Below I give you a complete, ready-to-run **non-deep-learning** solution:

* A **full Python script** that:

  * generates a realistic **synthetic** dataset (so you can test immediately),
  * trains several classical ML models (Logistic Regression, Random Forest, Gradient Boosting),
  * compares them with cross-validation,
  * shows metrics (accuracy, classification report, confusion matrix, feature importance, ROC AUC),
  * saves the best model to disk, and
  * provides an inference function to call from your app.

* Explanations for how to adapt this to **real medical data**, how to label training targets, and important safety/ethics cautions (this is **not** medical advice — always validate with clinicians).

**Important:** I was not able to execute the heavy training here. (If you want, you can run the script locally or on a server — instructions below.) I’ve kept the code clear and commented so you can run it, adapt it to your real dataset, and iterate.

---

# 1) What this model does (summary)

* Input: patient historical vitals (BP, pulse, temp, blood sugar, cholesterol flag), chronic disease flags (diabetes, hypertension, heart disease, etc.), **current emergency type** selected by neighbor/person (heart_attack, cardiac_arrest, severe_bleeding, anaphylaxis, seizure, none).
* Output: a **first-aid recommendation** (single label chosen from a set of actions such as `CPR_and_call_ambulance`, `Immediate_transport_stop_bleeding_and_call_ambulance`, `Administer_epinephrine_and_call_ambulance`, `Seizure_safe_position_monitor`, `Administer_oral_glucose_and_monitor`, `Basic_monitoring_and_follow_up`).
* Model types: classical ML (Logistic Regression, RandomForest, GradientBoosting). No deep learning.

---

# 2) Complete Python script (copy → save as `train_first_aid_model.py`)

```python
"""
train_first_aid_model.py

Full non-deep-learning example for predicting first-aid recommendation from:
 - vitals (blood pressure, pulse, temperature, blood sugar)
 - chronic disease flags (diabetes, heart_disease, etc.)
 - current emergency type (heart_attack, cardiac_arrest, severe_bleeding, anaphylaxis, seizure, none)

This script:
 - creates a synthetic dataset (so you can run immediately),
 - trains several classical ML models,
 - compares them with cross-validation,
 - produces metrics & plots (matplotlib),
 - saves the best trained pipeline to /mnt/data/first_aid_model.pkl (or local working dir),
 - provides an example inference function.

Requirements:
 pip install numpy pandas scikit-learn matplotlib joblib
"""

import numpy as np
import pandas as pd
import os
import joblib
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve

# -------------- 1. Create synthetic dataset (so you can run fast & test) --------------
def create_synthetic_dataset(n=2000, seed=42):
    rng = np.random.default_rng(seed)
    age = rng.integers(5, 95, size=n)
    bp_sys = rng.normal(120, 20, size=n).astype(int)
    bp_dia = rng.normal(80, 12, size=n).astype(int)
    pulse = np.clip(rng.normal(75, 15, size=n).astype(int), 30, 200)
    temp_f = np.round(rng.normal(98.2, 1.2, size=n), 1)
    blood_sugar = np.round(np.abs(rng.normal(100, 50, size=n)), 1)
    cholesterol = rng.choice([0,1], size=n, p=[0.7,0.3])  # 0 normal, 1 high

    disease_list = ['diabetes','hypertension','heart_disease','cancer','kidney_disease',
                    'liver_disease','asthma','copd','arthritis','thyroid_disorder','alzheimers','parkinsons']
    diseases = {}
    for d in disease_list:
        p = 0.12 if d in ['diabetes','hypertension'] else 0.05
        diseases[d] = rng.choice([0,1], size=n, p=[1-p, p])

    emergencies = ['heart_attack','cardiac_arrest','severe_bleeding','anaphylaxis','seizure','none']
    emergency = []
    for i in range(n):
        prob = [0.08, 0.04, 0.06, 0.03, 0.03, 0.76]
        if diseases['heart_disease'][i] == 1:
            prob = [0.15, 0.08, 0.04, 0.03, 0.02, 0.68]
        emergency.append(rng.choice(emergencies, p=prob))
    emergency = np.array(emergency)

    # mapping rules -> label (these are illustrative; replace with clinician-defined labels)
    def recommend_action(i):
        em = emergency[i]
        if em == 'cardiac_arrest':
            return 'CPR_and_call_ambulance'
        if em == 'heart_attack':
            if bp_sys[i] < 90 or pulse[i] > 120:
                return 'CPR_and_call_ambulance' if pulse[i] > 140 else 'Immediate_transport_stop_bleeding_and_call_ambulance'
            return 'Immediate_transport_stop_bleeding_and_call_ambulance'
        if em == 'severe_bleeding':
            return 'Immediate_transport_stop_bleeding_and_call_ambulance'
        if em == 'anaphylaxis':
            return 'Administer_epinephrine_and_call_ambulance'
        if em == 'seizure':
            return 'Seizure_safe_position_monitor'
        # non-emergency choices derived from vitals
        if blood_sugar[i] < 60:
            return 'Administer_oral_glucose_and_monitor'
        return 'Basic_monitoring_and_follow_up'

    labels = np.array([recommend_action(i) for i in range(n)])

    df = pd.DataFrame({
        'age': age,
        'bp_sys': bp_sys,
        'bp_dia': bp_dia,
        'pulse': pulse,
        'temp_f': temp_f,
        'blood_sugar': blood_sugar,
        'cholesterol_high': cholesterol,
        'emergency_type': emergency,
        'label': labels
    })
    for d in disease_list:
        df[d] = diseases[d]

    return df, disease_list

# -------------- 2. Preprocess and train models --------------
def train_and_evaluate(df, disease_list, save_path='first_aid_model.pkl'):
    # features & target
    feature_cols = ['age','bp_sys','bp_dia','pulse','temp_f','blood_sugar','cholesterol_high'] + disease_list + ['emergency_type']
    X = df[feature_cols]
    y = df['label']

    # label binarizer for later ROC plotting
    lb = LabelBinarizer()
    lb.fit(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    numeric_features = ['age','bp_sys','bp_dia','pulse','temp_f','blood_sugar']
    numeric_transformer = Pipeline([('scaler', StandardScaler())])
    categorical_features = ['emergency_type']
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')

    preprocessor = ColumnTransformer([
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ], remainder='passthrough')  # disease flags and cholesterol pass through

    # candidate models (adjust params as you like)
    candidates = {
        'LogisticRegression': Pipeline([('pre', preprocessor), ('clf', LogisticRegression(max_iter=800))]),
        'RandomForest': Pipeline([('pre', preprocessor), ('clf', RandomForestClassifier(n_estimators=150, random_state=42))]),
        'GradientBoosting': Pipeline([('pre', preprocessor), ('clf', GradientBoostingClassifier(n_estimators=150, random_state=42))])
    }

    results = []
    print("Cross-validating candidate models (3-fold CV):")
    for name, pipe in candidates.items():
        scores = cross_val_score(pipe, X_train, y_train, cv=3, scoring='accuracy', n_jobs=1)
        print(f" - {name}: CV accuracy = {scores.mean():.3f} ± {scores.std():.3f}")
        # fit on full training set (quick)
        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_test)
        test_acc = accuracy_score(y_test, preds)
        results.append((name, pipe, test_acc))

    # pick best by test accuracy
    results.sort(key=lambda t: t[2], reverse=True)
    best_name, best_pipe, best_acc = results[0]
    print(f"\nBest model: {best_name} with test accuracy {best_acc:.3f}")

    # evaluation
    y_pred = best_pipe.predict(X_test)
    print("\nClassification report (best model):")
    print(classification_report(y_test, y_pred))

    # confusion matrix
    cm = confusion_matrix(y_test, y_pred, labels=lb.classes_)
    fig, ax = plt.subplots(figsize=(8,6))
    ax.imshow(cm, interpolation='nearest')
    ax.set_title('Confusion Matrix')
    ax.set_xticks(range(len(lb.classes_))); ax.set_yticks(range(len(lb.classes_)))
    ax.set_xticklabels(lb.classes_, rotation=45, ha='right'); ax.set_yticklabels(lb.classes_)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i,j], ha='center', va='center')
    plt.tight_layout()
    plt.show()

    # feature importance: if tree-based
    if best_name in ['RandomForest', 'GradientBoosting']:
        # get transformed feature names
        cat_cols = list(best_pipe.named_steps['pre'].named_transformers_['cat'].get_feature_names_out(categorical_features))
        feature_names = numeric_features + cat_cols + ['cholesterol_high'] + disease_list
        X_test_trans = best_pipe.named_steps['pre'].transform(X_test)
        rf = best_pipe.named_steps['clf']
        # permutation importance (fast-ish)
        from sklearn.inspection import permutation_importance
        r = permutation_importance(rf, X_test_trans, y_test, n_repeats=8, random_state=42, n_jobs=1)
        imp = pd.Series(r.importances_mean, index=feature_names).sort_values(ascending=False)
        print("Top features (by permutation importance):")
        print(imp.head(12))
        fig, ax = plt.subplots(figsize=(8,4))
        imp.head(8).plot.bar(ax=ax)
        ax.set_title('Top feature importances (permutation)')
        plt.tight_layout()
        plt.show()
    else:
        # coefficients for logistic regression (one vs rest)
        try:
            lr = best_pipe.named_steps['clf']
            coefs = lr.coef_
            cat_cols = list(best_pipe.named_steps['pre'].named_transformers_['cat'].get_feature_names_out(categorical_features))
            feature_names = numeric_features + cat_cols + ['cholesterol_high'] + disease_list
            # for multiclass, show magnitude of first class
            coefs_magnitude = (np.abs(coefs)).sum(axis=0)
            coef_ser = pd.Series(coefs_magnitude[:len(feature_names)], index=feature_names).sort_values(ascending=False)
            print("Top features (by coefficient magnitude):")
            print(coef_ser.head(12))
            fig, ax = plt.subplots(figsize=(8,4)); coef_ser.head(8).plot.bar(ax=ax); plt.tight_layout(); plt.show()
        except Exception as e:
            print("Could not compute coefficients:", e)

    # ROC AUC per class (one-vs-rest) if probability available
    try:
        y_score = best_pipe.predict_proba(X_test)
        # if predict_proba returns list (one per class), convert to array
        if isinstance(y_score, list):
            y_score = np.vstack([arr[:,1] if arr.shape[1] > 1 else arr[:,0] for arr in y_score]).T
        # binarize y_test for ROC
        lb2 = LabelBinarizer(); Y_bin_test = lb2.fit_transform(y_test)
        if Y_bin_test.shape[1] == y_score.shape[1]:
            plt.figure(figsize=(8,6))
            for i, cls in enumerate(lb2.classes_):
                fpr, tpr, _ = roc_curve(Y_bin_test[:, i], y_score[:, i])
                auc = roc_auc_score(Y_bin_test[:, i], y_score[:, i])
                plt.plot(fpr, tpr, label=f"{cls} (AUC={auc:.2f})")
            plt.plot([0,1],[0,1], linestyle='--')
            plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC curves per class'); plt.legend(loc='lower right')
            plt.tight_layout(); plt.show()
    except Exception as e:
        print("Skipping ROC plotting (no predict_proba or mismatch):", e)

    # save best model
    save_dir = os.path.join(os.getcwd(), 'models')
    os.makedirs(save_dir, exist_ok=True)
    model_path = os.path.join(save_dir, save_path)
    joblib.dump(best_pipe, model_path)
    print("Saved best model to:", model_path)

    return model_path, best_pipe, lb

# -------------- 3. Example inference helper --------------
def load_and_predict(model_path, sample_dict):
    # sample_dict must contain all features used in training: age, bp_sys, bp_dia, pulse, temp_f, blood_sugar,
    # cholesterol_high, disease flags, emergency_type
    model = joblib.load(model_path)
    df_sample = pd.DataFrame([sample_dict])
    pred = model.predict(df_sample)[0]
    proba = None
    try:
        proba = model.predict_proba(df_sample)[0]
    except Exception:
        proba = None
    return pred, proba

# -------------- 4. If executed as main: run end-to-end --------------
if __name__ == "__main__":
    print("Creating synthetic dataset...")
    df, disease_list = create_synthetic_dataset(n=1500, seed=42)
    print("Dataset created. Class distribution:")
    print(df['label'].value_counts(normalize=True).round(3))
    model_path, best_pipe, lb = train_and_evaluate(df, disease_list, save_path='first_aid_model.pkl')

    # Example inference
    example = {
        'age': 68, 'bp_sys': 140, 'bp_dia': 90, 'pulse': 110, 'temp_f': 98.6, 'blood_sugar': 95.0,
        'cholesterol_high': 1, 'emergency_type': 'heart_attack'
    }
    # set disease flags (example)
    for d in disease_list:
        example[d] = 1 if d in ['hypertension','heart_disease'] else 0

    pred, proba = load_and_predict(model_path, example)
    print("Example prediction:", pred)
    if proba is not None:
        print("Probabilities (partial):")
        # print top 3 probable classes
        classes = best_pipe.named_steps['clf'].classes_
        proba_ser = pd.Series(proba, index=classes).sort_values(ascending=False)
        print(proba_ser.head(5))
```

---

# 3) How to run locally

1. Save the script above as `train_first_aid_model.py`.
2. Create a Python environment and install requirements:

   ```
   python -m venv venv
   source venv/bin/activate   # on Windows: venv\Scripts\activate
   pip install numpy pandas scikit-learn matplotlib joblib
   ```
3. Run:

   ```
   python train_first_aid_model.py
   ```
4. The script will save the trained pipeline to `./models/first_aid_model.pkl`. Use the `load_and_predict` helper to make predictions from your app.

---

# 4) Notes on dataset & labeling (very important)

* The model is only as good as the data and labels. For real-world usage:

  * **Collect real labeled examples** (each sample must have vitals + disease flags + emergency_type + an expert-provided first-aid recommendation).
  * **Labels must be defined by clinicians** (a set of safe first-aid actions, explicit protocol).
  * If you allow automated suggestions in real emergencies, you must follow local laws and medical device regulations — this system should be **decision-support only**; require human verification.
* If you have multiple valid first-aid steps simultaneously, you may want **multi-label** output instead of single-label. Use `MultiOutputClassifier` or `OneVsRestClassifier` with `MultiLabelBinarizer`.

---

# 5) Improvements & options

* **Multi-label** (recommended if you want to return several steps). Convert labels to binary vectors and use `OneVsRestClassifier(RandomForestClassifier(...))` or `sklearn.multioutput.MultiOutputClassifier`.
* **Time-series / daily vitals**: if you have sequences (vitals over last N days), you can compute summary statistics (min, max, mean, slope) and feed them to the model — still possible without deep learning.
* **Explainability**: add SHAP or LIME to explain per-prediction reasons (important for medical apps).
* **Validation**: hold out an external validation set, and have clinicians review model suggestions.
* **Deployment**: export the pipeline with `joblib.dump`, serve via FastAPI endpoint to the app, secured and audited.

---

# 6) UI integration idea (how neighbor enters code and emergency type)

* The neighbor enters the user's **access code** and selects current emergency type (from the UI tiles like in your screenshot).
* The backend fetches that user's historical vitals/diseases from DB, constructs the feature vector, runs the saved model pipeline, and returns the top recommendation + confidence + an explanation (feature contributions).
* Always show a disclaimer and a “Call Emergency Services” big button. For critical labels (e.g., `CPR_and_call_ambulance`), require the app to also show instructions (call 108 / local emergency) and guidance like “start chest compressions if trained”.

---

# 7) Safety & legal

* **Do not** rely solely on model for life-critical decisions. Always require human/clinician confirmation.
* Validate against real clinical data and get clinical sign-off before deploying in production.
* Consider logging, versioning, and an audit trail (which neighbor used it, timestamps) to help with liability and improvement.

---

# 8) If you want I can next:

* Convert the script into a **FastAPI** endpoint so your app can call `POST /predict` with patient code and emergency type and receive JSON result.
* Show how to convert to **multi-label** outputs (if you prefer multiple actionable steps).
* Add SHAP explanations to show which vitals/diseases pushed the recommendation in that case.
* Help prepare the database schema to store per-user daily vitals and chronic conditions for training on real data.

---

If you want, tell me:

1. Do you want a **single label** or **multi-label** (multiple actions) output?
2. Would you like me to generate a ready-to-deploy **FastAPI** server that loads the saved model and serves predictions to your app?

I'll produce the FastAPI code and example request/response format next — no waiting required.
